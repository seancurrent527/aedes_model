{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run this... download data files how they're stored in Google Drive (i.e. separate folders for Arizona, California, Florida, and Other States) \n",
    "# as well as summary files (AZ_states.csv and Other_states.csv)\n",
    "\n",
    "#Output: 3 dataframes (train_data, val_data, test_data)\n",
    "\n",
    "# First get file names organized \n",
    "# AZ_counties and Other_states are formatted differently\n",
    "# Start with Other_states\n",
    "names=pd.read_csv(\"Data/Other_states.csv\");\n",
    "#Header is: County, Lat, Long, Subfolder, Val, Actual State\n",
    "names=names.values[:];\n",
    "\n",
    "# Train and validate filenames will be the same\n",
    "train_filenames=[]\n",
    "test_filenames=[]\n",
    "\n",
    "train_city_names=[]\n",
    "test_city_names=[]\n",
    "\n",
    "\n",
    "for i in range(0,len(names)):\n",
    "    #California and Florida have their own subfolder, all other counties are in \"Other States\" folder\n",
    "    name=\"Data/\"+str(names[i,3])+\"/2011-2020_rcp45_CanESM2_\"+str(names[i,0])\n",
    "    #testing data has val=1\n",
    "    if names[i,4]==1:\n",
    "        try:\n",
    "            #does rcp45 exist for all counties? Not sure, so take rcp85 if it doesn't\n",
    "            pd.read_csv(name+\".csv\");\n",
    "            #file name we'll open later to concatenate data\n",
    "            test_filenames.append(name)\n",
    "            #keep track of city/state now to make it easier later\n",
    "            test_city_names.append(str(names[i,0])+\",\"+str(names[i,5]))\n",
    "        except:\n",
    "            name=\"Data/\"+str(names[i,3])+\"/2011-2020_rcp85_CanESM2_\"+str(names[i,0])\n",
    "            test_filenames.append(name)\n",
    "            test_city_names.append(str(names[i,0])+\",\"+str(names[i,5]))\n",
    "    #training data has val=0\n",
    "    #repeat same process as above for training/validation subsets\n",
    "    else:\n",
    "        try:\n",
    "            pd.read_csv(name+\".csv\");\n",
    "            train_filenames.append(name)\n",
    "            train_city_names.append(str(names[i,0])+\",\"+str(names[i,5]))\n",
    "        except:\n",
    "            name=\"Data/\"+str(names[i,3])+\"/2011-2020_rcp85_CanESM2_\"+str(names[i,0])\n",
    "            train_filenames.append(name)\n",
    "            train_city_names.append(str(names[i,0])+\",\"+str(names[i,5]))\n",
    "            \n",
    "\n",
    "#Same process as above for AZ counties. Only change is naming convention\n",
    "names=pd.read_csv(\"Data/AZ_states.csv\");\n",
    "names=names.values[:];\n",
    "\n",
    "for i in range(0,len(names)):\n",
    "    name=\"Data/Arizona/historical_rcp45_\"+str(names[i,1])+\"_\"+str(names[i,2])\n",
    "    #testing data has val=1\n",
    "    if names[i,4]==1:\n",
    "        try:\n",
    "            pd.read_csv(name+\".csv\");\n",
    "            test_filenames.append(name)\n",
    "            test_city_names.append(str(names[i,0])+\",\"+str(names[i,3]))\n",
    "        except:\n",
    "            name=\"Data/Arizona/historical_rcp85_\"+str(names[i,1])+\"_\"+str(names[i,2])\n",
    "            test_filenames.append(name)\n",
    "            test_city_names.append(str(names[i,0])+\",\"+str(names[i,3]))\n",
    "    #training data has val=0\n",
    "    else:\n",
    "        try:\n",
    "            pd.read_csv(name);\n",
    "            train_filenames.append(name)\n",
    "            train_city_names.append(str(names[i,0])+\",\"+str(names[i,3]))\n",
    "        except:\n",
    "            name=\"Data/Arizona/historical_rcp85_\"+str(names[i,1])+\"_\"+str(names[i,2])\n",
    "            train_filenames.append(name)\n",
    "            train_city_names.append(str(names[i,0])+\",\"+str(names[i,3]))\n",
    "\n",
    "#Save all of these as arrays for reference\n",
    "np.save('train_city_names',np.asarray(train_city_names))\n",
    "np.save('test_city_names',np.asarray(test_city_names))\n",
    "np.save('train_names',np.asarray(train_filenames))\n",
    "np.save('test_names',np.asarray(test_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "--Return--\n",
      "> <ipython-input-21-d855c50884aa>(113)<module>()->None\n",
      "-> pdb.set_trace()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    }
   ],
   "source": [
    "#function to get all of the indexes of the years I want\n",
    "get_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y]\n",
    "\n",
    "#load in all those arrays we saved last time\n",
    "#filenames\n",
    "train_names=np.load('train_names.npy', allow_pickle=True)\n",
    "test_names=np.load('test_names.npy', allow_pickle=True)\n",
    "#city names\n",
    "tr_cities=np.load('train_city_names.npy', allow_pickle=True)\n",
    "te_cities=np.load('test_city_names.npy', allow_pickle=True)\n",
    "\n",
    "train_cities=[]\n",
    "test_cities=[]\n",
    "val_cities=[]\n",
    "\n",
    "#indexes city names\n",
    "index=0\n",
    "\n",
    "#initialize dataframes so we can concatenate later\n",
    "train_fin=np.zeros((1,10))\n",
    "train_fin=pd.DataFrame(train_fin)\n",
    "val_fin=np.zeros((1,10))\n",
    "val_fin=pd.DataFrame(val_fin)\n",
    "for name in train_names:\n",
    "    #training data from 2011-2018, validation data from 2019-2020\n",
    "    tr_yr=2011\n",
    "    val_yr=2019\n",
    "    data=pd.read_csv(name+\".csv\")\n",
    "    data=data.values[:]\n",
    "    #Training data\n",
    "    while tr_yr<2019:\n",
    "        #AZ data doesn't have same years as other states, so check to see where beginning of the year is for each file\n",
    "        start=(get_indexes(tr_yr, data[:,0]))\n",
    "        #Concatenate training dataframe with 365 days for each year (ignore leap years)\n",
    "        if not np.isnan(data[start[0]:start[0]+365,:]).any():\n",
    "            train_fin=pd.concat([train_fin,pd.DataFrame(data[start[0]:start[0]+365,:])])\n",
    "            #Add associated city for each of rows corresponding to a year\n",
    "            for i in range(0,365):\n",
    "                train_cities.append(tr_cities[index])   \n",
    "        tr_yr+=1\n",
    "        \n",
    "    #Validation data\n",
    "    #Same process for validation data, just different years\n",
    "    while val_yr<2021:\n",
    "        start=(get_indexes(val_yr, data[:,0]))\n",
    "        if not np.isnan(data[start[0]:start[0]+365,:]).any():\n",
    "            val_fin=pd.concat([val_fin,pd.DataFrame(data[start[0]:start[0]+365,:])])\n",
    "            for i in range(0,365):\n",
    "                val_cities.append(tr_cities[index])   \n",
    "        val_yr+=1\n",
    "    index+=1\n",
    "\n",
    "#Testing counties are in different csv files, so handle those now. Same process as training/validation\n",
    "index=0\n",
    "test_fin=np.zeros((1,10))\n",
    "test_fin=pd.DataFrame(test_fin)\n",
    "for name in test_names:\n",
    "    #get all years (2011-2020) for testing counties. Narrow down later as needed\n",
    "    te_yr=2011\n",
    "    data=pd.read_csv(name+\".csv\")\n",
    "    data=data.values[:]\n",
    "        \n",
    "    #Testing data\n",
    "    while te_yr<2021:\n",
    "        start=(get_indexes(te_yr, data[:,0]))\n",
    "        if not np.isnan(data[start[0]:start[0]+365,:]).any():\n",
    "            test_fin=pd.concat([test_fin,pd.DataFrame(data[start[0]:start[0]+365,:])])\n",
    "            for i in range(0,365):\n",
    "                test_cities.append(te_cities[index])   \n",
    "        te_yr+=1\n",
    "    index+=1\n",
    "\n",
    "#Erase the junk first row used to initialize dataframes\n",
    "train_fin = train_fin.iloc[1:]\n",
    "test_fin = test_fin.iloc[1:]\n",
    "val_fin = val_fin.iloc[1:]\n",
    "\n",
    "#Convert city arrays to dataframes\n",
    "train_cities = pd.DataFrame(train_cities)\n",
    "test_cities = pd.DataFrame(test_cities)\n",
    "val_cities = pd.DataFrame(val_cities)\n",
    "\n",
    "#Make sure indexes align so dataframes can be concatenated\n",
    "train_fin.reset_index(drop=True, inplace=True)\n",
    "train_cities.reset_index(drop=True, inplace=True)\n",
    "test_fin.reset_index(drop=True, inplace=True)\n",
    "test_cities.reset_index(drop=True, inplace=True)\n",
    "val_fin.reset_index(drop=True, inplace=True)\n",
    "val_cities.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#concatenate and save :)\n",
    "train_data=pd.concat([train_cities, train_fin], axis=1)\n",
    "test_data=pd.concat([test_cities, test_fin], axis=1)\n",
    "validate_data=pd.concat([val_cities, val_fin], axis=1)\n",
    "\n",
    "short=train_data.values\n",
    "short=short[:,4:]\n",
    "short=short.astype(\"float32\")\n",
    "print(np.isnan(short).any())\n",
    "\n",
    "short=test_data.values\n",
    "short=short[:,4:]\n",
    "short=short.astype(\"float32\")\n",
    "print(np.isnan(short).any())\n",
    "\n",
    "\n",
    "short=validate_data.values\n",
    "short=short[:,4:]\n",
    "short=short.astype(\"float32\")\n",
    "print(np.isnan(short).any())\n",
    "\n",
    "\n",
    "pdb.set_trace()\n",
    "\n",
    "train_data.to_pickle(\"train_data\")\n",
    "test_data.to_pickle(\"test_data\")\n",
    "validate_data.to_pickle(\"val_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
